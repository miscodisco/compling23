{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce22cc66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. The same in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01d074",
   "metadata": {},
   "source": [
    "*Based on notebook by Lucas Champolion with some alterations from Mia Jacobsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec15f69",
   "metadata": {},
   "source": [
    "Now, let's do the same calculations again in Python. This is just a warm-up for the next exercise, where we will do the same calculations using millions of words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9496f",
   "metadata": {},
   "source": [
    "We start by counting the bigrams in our dataset. Don't worry if you don't fully understand every single part of the code we are using in this assignment (it uses more than just the basic Python we've introduced you to). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9d691",
   "metadata": {},
   "source": [
    "The point is that this code does the same as what you have just done yourself by hand: it counts every bigram and stores them in a way that makes it convenient for the computer to look up conditional probabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59718191",
   "metadata": {},
   "source": [
    "The code in this question and the next is modified from https://nlpforhackers.io/language-models/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b63dc",
   "metadata": {},
   "source": [
    "First, we import the Natural Language Toolkit (NLTK), which describes itself as a  \"leading platform for building Python programs to work with human language data\". While we're at it, we'll also import a few other packages we'll need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69b7ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:36.696285Z",
     "iopub.status.busy": "2023-02-21T11:27:36.695548Z",
     "iopub.status.idle": "2023-02-21T11:27:40.058113Z",
     "shell.execute_reply": "2023-02-21T11:27:40.057086Z",
     "shell.execute_reply.started": "2023-02-21T11:27:36.696225Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk # (1)\n",
    "from nltk import bigrams, trigrams #(2)\n",
    "import random # (3)\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d902b",
   "metadata": {},
   "source": [
    "Next, let's import a special Counter device that will make our code easier to write. A Counter is like a Python dictionary except that the value of each entry is an integer, initially zero, that can be incremented to count things. We will use it to count occurrences of bigrams and (later) trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58baed4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:42.161281Z",
     "iopub.status.busy": "2023-02-21T11:27:42.160604Z",
     "iopub.status.idle": "2023-02-21T11:27:42.168623Z",
     "shell.execute_reply": "2023-02-21T11:27:42.167410Z",
     "shell.execute_reply.started": "2023-02-21T11:27:42.161222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b539bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code blocks, referencing parts in parentheses, e.g. `(1)`._\n",
    "\n",
    "(1) When a word becomes highlighted in Jupyter notebooks, we know it has some special meaning in Python. In the case of this line, we see the `import` keyword come in to play. For simple Python functionality, we can write our own functions, do basic math, etc. However, not all programs need complex functionality (like working with human language data), so these are left to external libraries- like NLTK! Using the `import` statement, we're saying, \"Hey Python, we need a hand here- can you get the nltk library for us?\"\n",
    "\n",
    "(2) Here we see another special keyword, `from`. If we tried to say `import bigrams`, Python might not know what we're talking about. However, when we say `from`, we're saying to look specifically at some library for things WITHIN that library. For example, \"Hey Python! In the nltk library you just imported, please find the `bigrams` and `trigrams` information within it.\"\n",
    "\n",
    "(3) Here we see, similarly to (1), importing the `random` module, which lets us do things like generating random numbers. The `random` module can be thought of as an external file called `random.py`. When we import it and want to access any code within it, we have to use `random.thing`. For example, `random` has a function to generate random integers. Instead of saying `randint(0, 10)`, we need to remind python that we need the function from the `random` module. This is done with the `.` operator, such as writing `random.randint(0, 10)`. To use `bigrams`, for example, we'd have to write `nltk.bigrams.function()` everywhere we wanted to use the bigrams, but thanks to `(2)` we can say `bigrams.function()`!\n",
    "\n",
    "As an added note, python also has a special keyword `as` that can be used for imports. If I had a super long library name, like `florgleborglekorgle` and I didn't want to write that everywhere in my code, I can do `import florgleborglekorgle as fbk`. That way, I can write `fbk.function()` wherever I need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d804b2",
   "metadata": {},
   "source": [
    "We'll represent our dataset as a list of lists of strings. Each string is a word and each list is a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d62c37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:44.948395Z",
     "iopub.status.busy": "2023-02-21T11:27:44.947451Z",
     "iopub.status.idle": "2023-02-21T11:27:44.957765Z",
     "shell.execute_reply": "2023-02-21T11:27:44.956035Z",
     "shell.execute_reply.started": "2023-02-21T11:27:44.948317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seuss_dataset = [[\"I\", \"am\", \"Sam\"], \\\n",
    "                 [\"Sam\", \"I\", \"am\"], \\\n",
    "                 [\"I\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123d04c",
   "metadata": {},
   "source": [
    "To create our language model, we'll first count all the bigrams that occur in this dataset, and then convert our counts to probabilities. The Counter device will help us do this. We start by creating an empty Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62387590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8b7b04",
   "metadata": {},
   "source": [
    "The code in the next cell goes through the three sentences. For each of the sentences, it goes through the words in it, and uses the bigrams function from NLTK to access the bigrams in the sentence. It then uses the Counter device to count all the bigrams in the text. Note that \"i += 1\" is shorthand for \"i = i + 1\". Similarly for \"-=\", \"/=\", etc. We use w1 for the first word in a bigram and w2 for the second. The last line just causes the counter to print the outputs so you can inspect it, it doesn’t change the functionality of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2347d6ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:47.572289Z",
     "iopub.status.busy": "2023-02-21T11:27:47.571620Z",
     "iopub.status.idle": "2023-02-21T11:27:47.588869Z",
     "shell.execute_reply": "2023-02-21T11:27:47.587549Z",
     "shell.execute_reply.started": "2023-02-21T11:27:47.572233Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({None: Counter({'I': 2, 'Sam': 1}),\n",
       "         'I': Counter({'am': 2, 'do': 1}),\n",
       "         'am': Counter({'Sam': 1, None: 1}),\n",
       "         'Sam': Counter({None: 1, 'I': 1}),\n",
       "         'do': Counter({'not': 1}),\n",
       "         'not': Counter({'like': 1}),\n",
       "         'like': Counter({'green': 1}),\n",
       "         'green': Counter({'eggs': 1}),\n",
       "         'eggs': Counter({'and': 1}),\n",
       "         'and': Counter({'ham': 1}),\n",
       "         'ham': Counter({None: 1})})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is similar to last week when we made an empty dictionary to store word counts!\n",
    "seuss_model = Counter()\n",
    "\n",
    "for sentence in seuss_dataset: # (1)\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True): # (2)\n",
    "        if not w1 in seuss_model: # (3)\n",
    "            seuss_model[w1] = Counter()\n",
    "        seuss_model[w1][w2] += 1\n",
    "        \n",
    "seuss_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887f367",
   "metadata": {},
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block, referencing parts in parentheses, e.g. `(1)`._\n",
    "\n",
    "(1) Here we see the presence of a nested for loop, so a for loop within a for loop. This first for loop here simply says, \"for every sentence within `suess_dataset`, perform all the following code\".\n",
    "\n",
    "(2) This is where the trickiness of nested for loops comes in. This for loop is saying, \"for word1 and word2 in the bigrams of `sentence`, perform the following code\". Let's break this down.\n",
    "- `bigrams(sentence)` is saying to find all the bigrams in a following string `sentence`. This variable `sentence` comes from the outermost, or the first, for loop.\n",
    "- Because the function `bigrams` returns two values (versus, say, a `sum` function that returns one value), we use `w1, w2`. We're telling Python to store the first return value of `bigrams` into `w1`, and the second into `w2`. Neat!\n",
    "\n",
    "(3) Lastly, we're now checking if `w1` already exists in our variable `seuss_model`. If it doesn't, we go ahead and add it so that it exists. If it already exists, we don't want ot add it twice! Then we increase the count of two words occurring next to each other with that second to last line.\n",
    "\n",
    "An important distinction with nested for loops is that the innermost loop, `for w1, w2...` in this case, will always run through ALL iterations first. If we have 1 sentence, then the inner-most for loop runs once for every bigram. If we have two sentences, it runs once for every bigram in the first sentence, then once for every bigram in the second sentence... and so on and so forth. For `n` sentences, if each sentence has `x` bigrams, then we run the loop `n*x` times. Because the `seuss_model` is declared outside both for loops, it stores all of this data and never resets within the loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac59e63",
   "metadata": {},
   "source": [
    "Now, let's transform the counts to probabilities. We consider each word in turn and imagine it to be w1. Holding w1 fixed, we go through all the possible values for w2 and sum them up. Then we divide each of them by the sum. This way, all the w2 counts for a given w1 add up to 1. To do this, we use a for loop nested in another for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ef16e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:50.367526Z",
     "iopub.status.busy": "2023-02-21T11:27:50.366869Z",
     "iopub.status.idle": "2023-02-21T11:27:50.381193Z",
     "shell.execute_reply": "2023-02-21T11:27:50.380221Z",
     "shell.execute_reply.started": "2023-02-21T11:27:50.367470Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({None: Counter({'I': 0.6666666666666666, 'Sam': 0.3333333333333333}),\n",
       "         'I': Counter({'am': 0.6666666666666666, 'do': 0.3333333333333333}),\n",
       "         'am': Counter({'Sam': 0.5, None: 0.5}),\n",
       "         'Sam': Counter({None: 0.5, 'I': 0.5}),\n",
       "         'do': Counter({'not': 1.0}),\n",
       "         'not': Counter({'like': 1.0}),\n",
       "         'like': Counter({'green': 1.0}),\n",
       "         'green': Counter({'eggs': 1.0}),\n",
       "         'eggs': Counter({'and': 1.0}),\n",
       "         'and': Counter({'ham': 1.0}),\n",
       "         'ham': Counter({None: 1.0})})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w1 in seuss_model:\n",
    "    total_count = float(sum(seuss_model[w1].values()))\n",
    "    for w2 in seuss_model[w1]:\n",
    "        seuss_model[w1][w2] /= total_count\n",
    "\n",
    "seuss_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2cb92e",
   "metadata": {},
   "source": [
    "We can now use this model to estimate the probability that a word will occur next, given the previous word. As we said, this is called the \"maximum likelihood estimate\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46d6d4",
   "metadata": {},
   "source": [
    "Let’s make simple predictions with this language model. As before, we will start with one randomly selected word – “I”. We want our model to tell us what the next word might be. Remember that there are three bigrams whose first word is \"I\":\n",
    "\n",
    "`\n",
    "I am\n",
    "I am\n",
    "I do\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd5b30",
   "metadata": {},
   "source": [
    "Now, we will estimate the probabilities of the next word given that a randomly selected word is \"I\". When we did this ourselves, we noticed that this is the case in two of these three bigrams, so we estimated:\n",
    "\n",
    "$P(am | I)=2/3$ \n",
    "\n",
    "$P(do | I)=1/3$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88389d7",
   "metadata": {},
   "source": [
    "Here, w_1 is \"I\" and w_2 ranges over \"am\" and \"do\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c7fd4",
   "metadata": {},
   "source": [
    "Run the following command to ask the computer to do the same calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b92617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:53.668091Z",
     "iopub.status.busy": "2023-02-21T11:27:53.667422Z",
     "iopub.status.idle": "2023-02-21T11:27:53.676761Z",
     "shell.execute_reply": "2023-02-21T11:27:53.675208Z",
     "shell.execute_reply.started": "2023-02-21T11:27:53.668035Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am': 0.6666666666666666, 'do': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "words_after_I = dict(seuss_model[\"I\"])\n",
    "\n",
    "print(words_after_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c73f2",
   "metadata": {},
   "source": [
    "The computer could also have found these bigrams just by going through the text and looking for all the instances of \"I\". But arranging the dataset in a table of bigrams makes this lookup simpler and more efficient. Humans work the same way: it is often useful to arrange data in a format that makes looking up things simpler and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f5008",
   "metadata": {},
   "source": [
    "**Question 3.1** \n",
    "Which command would ask the computer to look up the probabilities of the next word given that a randomly selected word is \"am\"? (Hint: This is an easy question and its answer will look similar to the previous code box.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98e7aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:55.658471Z",
     "iopub.status.busy": "2023-02-21T11:27:55.657820Z",
     "iopub.status.idle": "2023-02-21T11:27:55.667368Z",
     "shell.execute_reply": "2023-02-21T11:27:55.665814Z",
     "shell.execute_reply.started": "2023-02-21T11:27:55.658414Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sam': 0.5, None: 0.5}\n"
     ]
    }
   ],
   "source": [
    "solution_q3_1 = dict(seuss_model[\"am\"])\n",
    "\n",
    "print(solution_q3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd18f5b",
   "metadata": {},
   "source": [
    "You will notice that the end of a sentence is represented by the special keyword None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf4880",
   "metadata": {},
   "source": [
    "**Question 3.2** \n",
    "Which command would ask the computer to look up the probabilities of the next word given that a randomly selected word is \"green\"? (Hint: This will also look similar to the previous code boxes.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e296885e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:27:57.821664Z",
     "iopub.status.busy": "2023-02-21T11:27:57.820866Z",
     "iopub.status.idle": "2023-02-21T11:27:57.828995Z",
     "shell.execute_reply": "2023-02-21T11:27:57.827839Z",
     "shell.execute_reply.started": "2023-02-21T11:27:57.821606Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eggs': 1.0}\n"
     ]
    }
   ],
   "source": [
    "solution_q3_2 = dict(seuss_model[\"green\"])\n",
    "\n",
    "print(solution_q3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fb42d",
   "metadata": {},
   "source": [
    "# 4. Trigrams using the Reuters Corpus\n",
    "(30 points -- autograded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2d7aa",
   "metadata": {},
   "source": [
    "In this problem, we will build a basic language model using trigrams of the Reuters corpus. (A \"corpus\" is a large and structured collection of texts. The plural of \"corpus\" is \"corpora\"). The Reuters corpus is a collection of 10,788 news documents totaling 1.3 million words. We can build a language model in a few lines of code using the NLTK package. Essentially, this code does the same as before, except with more words and with trigrams instead of bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6f3df",
   "metadata": {},
   "source": [
    "In probability notation, we write $P(w_n|w_{n-2},w_{n-1})$ for the probability that the next word will be $w_n$, given that the last two words were $w_{n-2}$ and $w_{n-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e4a35",
   "metadata": {},
   "source": [
    "The following box tells your notebook where to find the Reuters corpus as well as other corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dfaa4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:40:32.105569Z",
     "iopub.status.busy": "2023-02-21T11:40:32.104689Z",
     "iopub.status.idle": "2023-02-21T11:40:32.129662Z",
     "shell.execute_reply": "2023-02-21T11:40:32.129040Z",
     "shell.execute_reply.started": "2023-02-21T11:40:32.105491Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code in this question is based on https://nlpforhackers.io/language-models/\n",
    "\n",
    "# first we need to download any copora we might be interested in - so if you want gutenberg later, remember to download it like this\n",
    "\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe63f7",
   "metadata": {},
   "source": [
    "The following box specifies which corpus to use. We will start with the Reuters corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53e398",
   "metadata": {},
   "source": [
    "A later question will ask you to change the name of the corpus in this box. At this point, you don't need to change it. Just run the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f4e661f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:10.306658Z",
     "iopub.status.busy": "2023-02-21T11:42:10.306002Z",
     "iopub.status.idle": "2023-02-21T11:42:10.314389Z",
     "shell.execute_reply": "2023-02-21T11:42:10.313063Z",
     "shell.execute_reply.started": "2023-02-21T11:42:10.306602Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Available corpora include:\n",
    "# - reuters\n",
    "# - gutenberg\n",
    "# - webtext\n",
    "# - brown\n",
    "# - inaugural\n",
    "# - state_union\n",
    "\n",
    "# TO CHANGE THE CORPUS NAME, YOU WOULD EDIT THE FOLLOWING LINE\n",
    "from nltk.corpus import gutenberg as the_corpus \n",
    "\n",
    "# eg. to use gutenberg, you would change the previous line to: \n",
    "# from nltk.corpus import gutenberg as the_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53d54d",
   "metadata": {},
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block._\n",
    "\n",
    "Revisit the code explanation block in the import statements at the start of this assignment. Keep in mind the explanation of the `as` keyword as you look at the code below. When you change the corpus, we don't have to change everywhere `the_corpus` shows up- we would if we used something like `reuters.words()` instead of `the_corpus.words()`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b2824",
   "metadata": {},
   "source": [
    "By default, run this box without changes to load the Reuters corpus. Later on we will ask you to use other corpora. To download these corpora, you will then change the code as indicated and run it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738ccf1",
   "metadata": {},
   "source": [
    "Now let's print the first few words of the corpus just to make sure everything is working as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80e3a921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:15.194756Z",
     "iopub.status.busy": "2023-02-21T11:42:15.194538Z",
     "iopub.status.idle": "2023-02-21T11:42:15.201881Z",
     "shell.execute_reply": "2023-02-21T11:42:15.201317Z",
     "shell.execute_reply.started": "2023-02-21T11:42:15.194736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of corpus: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"Beginning of corpus:\", the_corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820cb496",
   "metadata": {},
   "source": [
    "\n",
    "If you have loaded the Reuters corpus, you should see something like this: Beginning of corpus: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e5f2d",
   "metadata": {},
   "source": [
    "The following code is very similar to the one in the previous question, and does essentially the same thing, but with trigrams instead of bigrams: construct a big table where each row is the previous two words, and each column is the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ada28",
   "metadata": {},
   "source": [
    "Let's create an empty language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441d204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140d4486",
   "metadata": {},
   "source": [
    "Next, we go through the corpus and count the trigrams we see. Depending on how large it is, this might need a lot of computer memory (about 700 MB for the reuters corpus). You can see in the top right corner how much memory you are using. Normally, this shouldn't be a problem, but if you run into any issues, let us know via the Padlet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e59cbd2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:21.105305Z",
     "iopub.status.busy": "2023-02-21T11:42:21.104530Z",
     "iopub.status.idle": "2023-02-21T11:42:29.673741Z",
     "shell.execute_reply": "2023-02-21T11:42:29.672933Z",
     "shell.execute_reply.started": "2023-02-21T11:42:21.105250Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_model = Counter()\n",
    "\n",
    "for sentence in the_corpus.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        # this means we now use two words at a time to predict the next\n",
    "        if not (w1,w2) in corpus_model:\n",
    "            corpus_model[w1,w2] = Counter()\n",
    "        corpus_model[w1,w2][w3] += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2eb3a1",
   "metadata": {},
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block._\n",
    "\n",
    "Once again we have a nested for loop performing an operation on every sentence. The structure is the same as the use of bigrams in our `seuss_model` previously in the assignment, but now we're using trigrams!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5ef62",
   "metadata": {},
   "source": [
    "Now, let's transform the counts to probabilities. As before, we use a for loop nested in another for loop. The only difference is we have trigrams rather than bigrams, so we have w1, w2, and w3 instead of just w1 and w2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9044120f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:35.516010Z",
     "iopub.status.busy": "2023-02-21T11:42:35.515804Z",
     "iopub.status.idle": "2023-02-21T11:42:36.589932Z",
     "shell.execute_reply": "2023-02-21T11:42:36.589136Z",
     "shell.execute_reply.started": "2023-02-21T11:42:35.515990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for (w1,w2) in corpus_model:\n",
    "    total_count = float(sum(corpus_model[w1,w2].values()))\n",
    "    for w3 in corpus_model[w1,w2]:\n",
    "        corpus_model[w1,w2][w3] /= total_count\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636c951",
   "metadata": {},
   "source": [
    "Let’s make simple predictions with this language model. We will start with two simple words – “today the”. We want our model to tell us what it thinks the next word will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3fb7d",
   "metadata": {},
   "source": [
    "Let's try this out with the words \"today\" and \"the\". See what happens when you replace one or both of these words, e.g. replacing \"the\" with \"a\" or replacing \"today\" with \"on\" or \"yesterday\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fbb5a24e-9b9b-429f-a907-42b96de61124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:36.591608Z",
     "iopub.status.busy": "2023-02-21T11:42:36.591384Z",
     "iopub.status.idle": "2023-02-21T11:42:36.596962Z",
     "shell.execute_reply": "2023-02-21T11:42:36.596395Z",
     "shell.execute_reply.started": "2023-02-21T11:42:36.591588Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'blood': 1.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_model['yesterday','the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e03629a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:36.597853Z",
     "iopub.status.busy": "2023-02-21T11:42:36.597655Z",
     "iopub.status.idle": "2023-02-21T11:42:36.614824Z",
     "shell.execute_reply": "2023-02-21T11:42:36.614001Z",
     "shell.execute_reply.started": "2023-02-21T11:42:36.597835Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('conquering'|'today','the') = 0.5\n",
      "P('whole'|'today','the') = 0.5\n"
     ]
    }
   ],
   "source": [
    "word1 = \"today\"\n",
    "word2 = \"the\"\n",
    "\n",
    "# The following code looks up the probabilities of all the words that follow these two words, and prints them \n",
    "# in a human-readable format.\n",
    "if corpus_model == None: # (1)\n",
    "    pass\n",
    "elif (word1,word2) not in corpus_model: # (2)\n",
    "     print(\"There are no occurrences of '\"+word1+\" \"+word2+\"' in the corpus!\")\n",
    "#    random_text = random.sample(list(corpus_model), 1)[0]\n",
    "#    print(\"There are no occurrences of '\"+word1+\" \"+word2+\"' in the corpus! Using random words '\"+random_text[0]+\" \"+random_text[1]+\"' instead.\")\n",
    "#    text[0]=random_text[0]\n",
    "#    text[1]=random_text[1]\n",
    "else: # (3)\n",
    "    values = corpus_model[word1, word2]\n",
    "    sorted_result = sorted(values.items(), key=lambda x: (-x[1], x[0].lower())) # (4)\n",
    "    for entry in sorted_result: # (5)\n",
    "        word=entry[0]\n",
    "        prob=entry[1]\n",
    "        prob_rounded=round(entry[1],3)\n",
    "        if prob == prob_rounded:\n",
    "            print(\"P('\"+entry[0]+\"'|'\"+word1+\"','\"+word2+\"') = \"+str(entry[1]))\n",
    "        else:\n",
    "            print(\"P('\"+entry[0]+\"'|'\"+word1+\"','\"+word2+\"') = \"+str(entry[1])+\" (rounded: \"+str(round(entry[1],3))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f26116",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block, referencing parts in parentheses, e.g. `(1)`._\n",
    "\n",
    "(1) This is a simple `if` statement that says \"if `corpus_model` doesn't exist, do nothing\". Since we can't have empty `if` statements in Python, amongst other things, we use the keyword `pass`.\n",
    "\n",
    "(2) This `elif` statement executes only if the above `if` statement does not. It simply checks if `word1` and `word2` exist as a pair inside of the previous generated `corpus_model` and prints this out if it doesn't. Note that `word1` and `word2` are values declared at the top of the code cell. Change them to non-words and see what prints!\n",
    "\n",
    "(3) If neither the above `if` or `elif` statements execute, then this `else` block will execute. For the two given words, we store the value in `corpus_model` into the variable called `values.`\n",
    "\n",
    "(4) Here, we are storing something inside the variable `sorted_values`. All in all, we are sorting the the list `values.items()` (feel free to write `print(values.items())` above this and see what it shows) according to the formula after `key=`. Don't stress too much about this though, it is just the way you tell Python to put it in descending order (i.e., if you ever need something like this again, use google or copy/paste this line)\n",
    "\n",
    "(5) And that all brings us to our for loop! For every item in our `sorted_result` list, which we call `entry` here, `entry[0]`, to be the word being compared and `entry[1]` which gets the probably of that. We then print this result, showing the word, our `word1` and `word2` above, as well as its probability. See if you can see where each of these variables print out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1361701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:58.602275Z",
     "iopub.status.busy": "2023-02-21T11:42:58.602066Z",
     "iopub.status.idle": "2023-02-21T11:42:58.609743Z",
     "shell.execute_reply": "2023-02-21T11:42:58.609100Z",
     "shell.execute_reply.started": "2023-02-21T11:42:58.602255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For ease of use, I've made it a function\n",
    "# notice that I could just copy/paste almost all of the code, because my function inputs are the same as how we defined the words in the previous cell\n",
    "# meaning that functions can usually be built from code you already have with minor alterations!\n",
    "\n",
    "def trigram_probs(word1, word2):\n",
    "    # for function purposes, print statements become saved as an output which I then return\n",
    "    output = []\n",
    "    if corpus_model == None:\n",
    "        pass\n",
    "    elif (word1,word2) not in corpus_model: \n",
    "         output = (\"There are no occurrences of '\"+word1+\" \"+word2+\"' in the corpus!\")\n",
    "    else: \n",
    "        values = corpus_model[word1, word2]\n",
    "        sorted_result = sorted(values.items(), key=lambda x: (-x[1], x[0].lower())) \n",
    "        for entry in sorted_result: # (5)\n",
    "            word=entry[0]\n",
    "            prob=entry[1]\n",
    "            prob_rounded=round(entry[1],3)\n",
    "            if prob == prob_rounded:\n",
    "                output.append(\"P('\"+entry[0]+\"'|'\"+word1+\"','\"+word2+\"') = \"+str(entry[1]))\n",
    "            else:\n",
    "                output.append(\"P('\"+entry[0]+\"'|'\"+word1+\"','\"+word2+\"') = \"+str(entry[1])+\" (rounded: \"+str(round(entry[1],3))+\")\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5229c",
   "metadata": {},
   "source": [
    "**Bonus Q.** Take the function from above which returns a list of the previous print statements and instead make it return the probability of a specific word given word1 and word2 (i.e., make a function that gives you the answer to the following 3 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3cf545c-8f18-4b8c-8ca8-0aa1cc7b5d52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:58.610697Z",
     "iopub.status.busy": "2023-02-21T11:42:58.610514Z",
     "iopub.status.idle": "2023-02-21T11:42:58.631030Z",
     "shell.execute_reply": "2023-02-21T11:42:58.630152Z",
     "shell.execute_reply.started": "2023-02-21T11:42:58.610679Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ice never follows today the'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_bonus(word1,word2,word3):\n",
    "    if corpus_model == None:\n",
    "        pass\n",
    "    elif (word1,word2) not in corpus_model: \n",
    "         output = (f\"There are no occurrences of {word1} and {word2} in the corpus!\")\n",
    "    else:\n",
    "        values = corpus_model[word1, word2]\n",
    "        sorted_result = sorted(values.items(), key=lambda x: (-x[1], x[0].lower())) \n",
    "        \n",
    "        for entry in sorted_result: # (5)\n",
    "            word=entry[0]\n",
    "            if word == word3:\n",
    "                prob=entry[1]\n",
    "                output=round(prob,3)\n",
    "    return output\n",
    "\n",
    "def super_duper_fun_bonus_exercise(word1,word2,word3):\n",
    "    if corpus_model == None:\n",
    "        pass\n",
    "    elif (word1,word2) not in corpus_model: \n",
    "         output = (f\"There are no occurrences of {word1} and {word2} in the corpus!\")\n",
    "            \n",
    "    else:\n",
    "        results = dict(corpus_model[word1,word2])\n",
    "        if not word3 in results:\n",
    "            output = f\"{word3} never follows {word1} {word2}\"\n",
    "        else:\n",
    "            output = round(results[word3],3)\n",
    "    return output\n",
    "\n",
    "super_duper_fun_bonus_exercise('today', 'the', 'ice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "110a3cc4-2be3-42cb-a950-7f041d4b772f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:58.655758Z",
     "iopub.status.busy": "2023-02-21T11:42:58.655212Z",
     "iopub.status.idle": "2023-02-21T11:42:58.665275Z",
     "shell.execute_reply": "2023-02-21T11:42:58.664540Z",
     "shell.execute_reply.started": "2023-02-21T11:42:58.655708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are no occurrences of today and we in the corpus!'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bonus_bonus_func(word1, word2):\n",
    "    if corpus_model == None:\n",
    "        pass\n",
    "    elif (word1,word2) not in corpus_model: \n",
    "         output = (f\"There are no occurrences of {word1} and {word2} in the corpus!\")\n",
    "    else:\n",
    "        values = corpus_model[word1, word2]\n",
    "        sorted_result = sorted(values.items(), key=lambda x: (-x[1], x[0].lower())) \n",
    "        output = sorted_result[0][0]\n",
    "    return output\n",
    "\n",
    "bonus_bonus_func('today','we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2314fa4b-6b26-4168-bf82-45000c53c299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:42:58.666313Z",
     "iopub.status.busy": "2023-02-21T11:42:58.666086Z",
     "iopub.status.idle": "2023-02-21T11:42:58.769064Z",
     "shell.execute_reply": "2023-02-21T11:42:58.767532Z",
     "shell.execute_reply.started": "2023-02-21T11:42:58.666291Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'output' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msimple_bonus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36msimple_bonus\u001b[0;34m(word1, word2, word3)\u001b[0m\n\u001b[1;32m     13\u001b[0m             prob\u001b[38;5;241m=\u001b[39mentry[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m             output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(prob,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutput\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'output' referenced before assignment"
     ]
    }
   ],
   "source": [
    "simple_bonus('today', 'the', 'Bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659c59e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-21T11:42:58.769558Z",
     "iopub.status.idle": "2023-02-21T11:42:58.769796Z",
     "shell.execute_reply": "2023-02-21T11:42:58.769681Z",
     "shell.execute_reply.started": "2023-02-21T11:42:58.769669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def bonus_solution(word1, word2, word3):\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7580ca",
   "metadata": {},
   "source": [
    "**Question 4.1.** What is the probability $P(Bank|today,the)$? You can give the solution rounded to three decimals. This is an easy question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60da4ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-21T11:42:58.770679Z",
     "iopub.status.idle": "2023-02-21T11:42:58.770938Z",
     "shell.execute_reply": "2023-02-21T11:42:58.770825Z",
     "shell.execute_reply.started": "2023-02-21T11:42:58.770812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_q4_1 = trigram_probs('today', 'the')\n",
    "\n",
    "solution_q4_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6b355",
   "metadata": {},
   "source": [
    "**Question 4.2.** Now, what is the probability $P(company|today,the)$? You can give the solution rounded to three decimals. This is an easy question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a5fb89f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:43:08.778879Z",
     "iopub.status.busy": "2023-02-21T11:43:08.778055Z",
     "iopub.status.idle": "2023-02-21T11:43:08.787975Z",
     "shell.execute_reply": "2023-02-21T11:43:08.786691Z",
     "shell.execute_reply.started": "2023-02-21T11:43:08.778819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'company never follows today the'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution_q4_2 = super_duper_fun_bonus_exercise('today', 'the', 'company')\n",
    "\n",
    "solution_q4_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebd445",
   "metadata": {},
   "source": [
    "**Question 4.3.** Now, what is the probability $P(reported|the,company)$? This will require you to change the words \"today\" and \"the\" in the cell above, running it again, and looking up the answer in the output. You can give the solution rounded to three decimals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "37635c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:43:16.495701Z",
     "iopub.status.busy": "2023-02-21T11:43:16.495494Z",
     "iopub.status.idle": "2023-02-21T11:43:16.500021Z",
     "shell.execute_reply": "2023-02-21T11:43:16.499446Z",
     "shell.execute_reply.started": "2023-02-21T11:43:16.495683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_q4_3 = trigram_probs('the', 'company')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ecef521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:50:10.645354Z",
     "iopub.status.busy": "2023-02-21T11:50:10.645152Z",
     "iopub.status.idle": "2023-02-21T11:50:11.017318Z",
     "shell.execute_reply": "2023-02-21T11:50:11.016266Z",
     "shell.execute_reply.started": "2023-02-21T11:50:10.645336Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she loves the Admiral was putting a stone in a cottage for Susan and her mother, and, behold, one ram, and to pour upon the name of the family property ,) she was going off into the way: 18 And Rehoboam went to the city whither I have heard a little too hasty decision, and put it upon many waters, ( What were you, and a thick cloud, a face looking in that time.\n"
     ]
    }
   ],
   "source": [
    "# code courtesy of https://nlpforhackers.io/language-models/\n",
    "\n",
    "# first let's reload the corpora and tokenizer in case user has reset the kernel after the previous question\n",
    "\n",
    "#nltk.data.path=[str(Path.home())+'/groupshare/nltk_data']\n",
    "\n",
    "# starting words\n",
    "text = [\"she\", \"loves\"]\n",
    "sentence_finished = False\n",
    "\n",
    "if (text[0],text[1]) not in corpus_model: #1\n",
    "    print(\"There are no occurrences of '\"+text[0]+\" \"+text[1]+\"' in the corpus!\")\n",
    "else: \n",
    "    while not sentence_finished: #2\n",
    "        # select a random probability threshold  \n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        for word in corpus_model[tuple(text[-2:])].keys(): #3\n",
    "            accumulator += corpus_model[tuple(text[-2:])][word]\n",
    "            # select words that are above the probability threshold\n",
    "            if accumulator >= r: #4\n",
    "                text.append(word)\n",
    "                break\n",
    "\n",
    "        if text[-2:] == [None, None]: #5\n",
    "            sentence_finished = True\n",
    "\n",
    "    text = ' '.join([t for t in text if t]) #6\n",
    "    text = re.sub(r'\\s([?.!\",\\';:‘](?:\\s|$))', r'\\1', text) # removes whitespaces before punctuation\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "91f6d05a-170f-4623-9f12-634b12ee0724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:51:57.273985Z",
     "iopub.status.busy": "2023-02-21T11:51:57.273310Z",
     "iopub.status.idle": "2023-02-21T11:52:14.872684Z",
     "shell.execute_reply": "2023-02-21T11:52:14.871854Z",
     "shell.execute_reply.started": "2023-02-21T11:51:57.273928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "with open ('mycorpus.txt', 'r') as file_in:\n",
    "    corpus_model = Counter()\n",
    "    for line in file_in:\n",
    "        sentence = word_tokenize(line)\n",
    "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "            if not (w1,w2) in corpus_model:\n",
    "                corpus_model[w1,w2] = Counter()\n",
    "            corpus_model[w1,w2][w3] += 1\n",
    "\n",
    "    for (w1,w2) in corpus_model:\n",
    "        total_count = float(sum(corpus_model[w1,w2].values()))\n",
    "        for w3 in corpus_model[w1,w2]:\n",
    "            corpus_model[w1,w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8a538b31-708d-44b2-a355-823f2732991c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T11:54:04.620351Z",
     "iopub.status.busy": "2023-02-21T11:54:04.619937Z",
     "iopub.status.idle": "2023-02-21T11:54:04.673253Z",
     "shell.execute_reply": "2023-02-21T11:54:04.672029Z",
     "shell.execute_reply.started": "2023-02-21T11:54:04.620322Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat it?\n"
     ]
    }
   ],
   "source": [
    "text = [\"I\", \"eat\"]\n",
    "sentence_finished = False\n",
    "\n",
    "if (text[0],text[1]) not in corpus_model:\n",
    "    print(\"There are no occurrences of '\"+text[0]+\" \"+text[1]+\"' in the corpus!\")\n",
    "else: \n",
    "    while not sentence_finished:\n",
    "        # select a random probability threshold  \n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        for word in corpus_model[tuple(text[-2:])].keys():\n",
    "            accumulator += corpus_model[tuple(text[-2:])][word]\n",
    "            # select words that are above the probability threshold\n",
    "            if accumulator >= r:\n",
    "                text.append(word)\n",
    "                break\n",
    "\n",
    "        if text[-2:] == [None, None]:\n",
    "            sentence_finished = True\n",
    "    text = ' '.join([t for t in text if t])\n",
    "    text = re.sub(r'\\s([?.!\",\\';:‘](?:\\s|$))', r'\\1', text) # removes whitespaces before punctuation\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a5fc0-3996-48b8-96de-488dfc29447d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
