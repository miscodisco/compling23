{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "jws1F8wMj1Nq"
   },
   "source": [
    "### HW9.2 Topic Modelling - and more - with Gensim!\n",
    "\n",
    "This tutorial will attempt to walk you through the entire process of analysing your text - from pre-processing to creating your topic models and visualising them. \n",
    "\n",
    "python offers a very rich suite of NLP and CL tools, and we will illustrate these to the best of our capabilities.\n",
    "Let's start by setting up our imports.\n",
    "\n",
    "We will be needing: \n",
    "```\n",
    "- Gensim\n",
    "- matplotlib\n",
    "- spaCy\n",
    "- pyLDAVis\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install gensim\n",
    "%pip install spacy\n",
    "%pip install pyLDAvis\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttZ4CjtDj1Nr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRKee2ZAj1Ns"
   },
   "source": [
    "For this tutorial, you have the option to play around with four different coropa which can be found in the 'corpora' folder here on UCloud. They are:\n",
    "- ABC news headlines\n",
    "- Scientific research articles\n",
    "- Ted Talk transcripts\n",
    "- Book synopses\n",
    "\n",
    "These are all relatively manageable in size and in so-called csv format (comma-separated values format).\n",
    "However, you will still have to do a little work with loading the files, setting up the dataframes, and running the topic models on the right columns. \n",
    "Addtionally, since this is a combination of code I (Mia) have adapted for our files and code from other places, not everything will necessarilly work immediatly - it might take some tweaking and such. Please do ask if you need help or don't understand the code :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqMzoUvAj1Ns",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/work/463910/compling23/HW9_material/corpora/'\n",
    "file = data_dir + 'book_synopses.csv' # you can change this to the data you want\n",
    "\n",
    "data = pd.read_csv(file)\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell wo have a look around in the different columns and rows of the data frame \n",
    "# Do you remember what we learned last semester? ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeFJKjMBj1Ns"
   },
   "source": [
    "### Pre-processing data!\n",
    "\n",
    "It's been often said in Machine Learning and NLP algorithms - garbage in, garbage out. We can't have state-of-the-art results without data which is aa good. Let's spend this section working on cleaning and understanding our data set.\n",
    "NTLK is usually a popular choice for pre-processing - but is rather [outdated](https://explosion.ai/blog/dead-code-should-be-buried). We will be checking out spaCy, an industry grade text-processing package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0liQHhFLj1Nt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSKsonKRj1Nt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = str(data['description'])\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJP6Elg7j1Nt"
   },
   "source": [
    "Voila! With the `English` pipeline, all the heavy lifting has been done. Let's see what went on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4drratxj1Nu"
   },
   "source": [
    "It seems like nothing, right? But spaCy's internal data structure has done all the work for us. Let's see how we can create our corpus. You can check out what a gensim corpus looks like [here](google.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByLafPM8j1Nu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what does this cell do? does it work optimally for our texts? play around with it :) \n",
    "\n",
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8A93fXIj1Nu"
   },
   "source": [
    "And this is the magic of spaCy - just like that, we've managed to get rid of stopwords, punctuation markers, and added the lemmatized word. There's lot more we can do with spaCy which I would really recommend checking out.\n",
    "\n",
    "Sometimes topic models make more sense when 'New' and 'York' are treated as 'New_York' - we can do this by creating a bigram model and modifying our corpus accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYOlqCXMj1Nu"
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVlKfSsbj1Nv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQKZDWXEj1Nv"
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w2wWJzij1Nv"
   },
   "source": [
    "We're now done with a very important part of any text analysis - the data cleaning and setting up of corpus. It must be kept in mind that we created the corpus the way we did because that's how gensim requires it - most algorithms still require one to clean the data set the way we did, by removing stop words and numbers, adding the lemmatized form of the word, and using bigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4HPzoOaj1Nv"
   },
   "source": [
    "### LSI\n",
    "\n",
    "LSI stands for Latent Semantic Indeixing - it is a popular information retreival method which works by decomposing the original matrix of words to maintain key topics. Gensim's implementation uses an SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssDvmu8Zj1Nv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkaTH3dIj1Nv",
    "outputId": "e0a4d093-b355-4c37-f192-e4b7d96c22f7"
   },
   "outputs": [],
   "source": [
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SacvlP6Vj1Nv"
   },
   "source": [
    "### HDP\n",
    "\n",
    "HDP, the Hierarchical Dirichlet process is an unsupervised topic model which figures out the number of topics on it's own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCMCF8j_j1Nv"
   },
   "outputs": [],
   "source": [
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVWhtl67j1Nv",
    "outputId": "b64f819b-3ba1-4ced-c6db-4edc068160dc"
   },
   "outputs": [],
   "source": [
    "hdpmodel.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t_AhxVxj1Nw"
   },
   "source": [
    "### LDA\n",
    "\n",
    "LDA, or Latent Dirichlet Allocation is arguably the most famous topic modelling algorithm out there. Out here we create a simple topic model with 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLNY7bArj1Nw"
   },
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjUyRfbWj1Nw",
    "outputId": "862f4f1a-6a2f-4fb1-d9dd-d0e0b6529fad"
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_kJVySpj1Nw"
   },
   "source": [
    "### pyLDAvis \n",
    "\n",
    "Thanks to pyLDAvis, we can visualise our topic models in a really handy way. All we need to do is enable our notebook and prepare the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEpAR6FOj1Nw",
    "outputId": "80777a6d-84dd-479b-a65b-b6b11b909f01"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y27kr590j1Nw"
   },
   "source": [
    "### Round-up\n",
    "\n",
    "Okay - so what have we learned so far? \n",
    "By using spaCy, we cleaned up our data super fast. It's worth noting that by running our doc through the pipeline we also know about every single words POS-tag and NER-tag. This is useful information and we can do some funky things with it! I would highly recommend going through [this](https://github.com/explosion/spacy-notebooks) repository to see examples of hands-on spaCy usage.\n",
    "\n",
    "As for gensim and topic modelling, it's pretty easy to see how well we could create our topic models. Now the obvious next question is - how do we use these topic models? The [news classification notebook](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/gensim_news_classification.ipynb) in the Gensim [notebooks](https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks) directory is a good example of how we can use topic models in a practical scenario.\n",
    "\n",
    "We will continue this tutorial by demonstrating a newer topic modelling features of gensim - in particular, Topic Coherence. \n",
    "\n",
    "### Topic Coherence\n",
    "\n",
    "Topic Coherence is a new gensim functionality where we can identify which topic model is 'better'. \n",
    "By returning a score, we can compare between different topic models of the same. We use the same example from the news classification notebook to plot a graph between the topic models we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ccqx2CNj1Nw"
   },
   "outputs": [],
   "source": [
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smaCz9noj1Nw"
   },
   "outputs": [],
   "source": [
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=texts, dictionary=dictionary, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG6Inp4sj1Nw"
   },
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, indices):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    \n",
    "    coherences: list of coherence values\n",
    "    indices: Indices to be used to mark bars. Length of this and coherences should be equal.\n",
    "    \"\"\"\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YEeD2s3j1Nw",
    "outputId": "43880590-9d68-4d32-fdb8-4e1f74ae1500"
   },
   "outputs": [],
   "source": [
    "evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence],\n",
    "                   ['LSI', 'HDP', 'LDA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCb9YS18j1Nx"
   },
   "source": [
    "We can see that topic coherence helped us get past manually inspecting our topic models - we can now keep fine tuning our models and compare between them to see which has the best performance. \n",
    "\n",
    "This also brings us to the end of the runnable part of this tutorial - we will continue however by briefly going over two more Jupyter notebooks I have previously worked on - mainly, [Dynamic Topic Modelling](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb) and [Document Word Coloring](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
