{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuqT7ofCAEEO"
   },
   "source": [
    "# Assignment 12: Precision and recall, Boolean search, PageRank\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX8nxvOyAEEP"
   },
   "source": [
    "This problem set consists of several different sections. Some of them involve coding, others require you to specify a number or string, and others are free response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pTV-uLsAEEQ"
   },
   "source": [
    "At the end of this lesson, you will be able to:\n",
    "\n",
    "\n",
    "- Compute the precision and recall of a simple search or similar process\n",
    "- Construct and execute simple Boolean searches on a given text\n",
    "- Run the PageRank algorithm, modify its input, and evaluate the consequences on its output\n",
    "\n",
    "\n",
    "While you work on the assignment, save early and often. If you get a \"dead kernel\" error, click on \"Kernel\", then \"Restart\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG3F3qGId--X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Preliminary: install and load nltk (Natural Language Toolkit)\n",
    "\n",
    "!pip install nltk\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpDhS66VAEER"
   },
   "source": [
    "# 1. Precision and recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZbAFwg9AEER"
   },
   "source": [
    "*Look into what precision and recall mean [here](https://en.wikipedia.org/wiki/Precision_and_recall)* \n",
    "\n",
    "**Question 1.1** You have 100 books on your bookshelf, of which 60 are about cats and 40 are about dogs. You build a catalog for your little collection. Your friend uses this catalog to search your collection for books about cats. The catalog returns the titles of 50 of the books on your shelf. Of the 50 titles it returns, 40 are about cats and the rest are about dogs. What is the **precision** of your catalog in the case of this query? Give the result either as a fraction of integers or as a decimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OK_agaDAEER",
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_q1_1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUfuq_c6AEER"
   },
   "source": [
    "**Question 1.2** And what is the **recall** of your catalog in the case of this query? Give the result either as a fraction of integers or as a decimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnpIJxAkAEER",
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_q1_2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ou81D9CYB598"
   },
   "source": [
    "# 2. Boolean search using five documents on Star Wars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUiZR5wIB599"
   },
   "source": [
    "### Making the corpus\n",
    "Below you will find five brief \"documents\" (text snippets) on the Star Wars movies, taken from Wikipedia. Don't forget to run the code in each cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNMXc31DB59-",
    "outputId": "a0bd7d01-ba3c-4c5c-d439-79cc3e0b85c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc1 = \"Frank Oz provided Yoda's voice with each film and spent his skills to be a puppeteer in the original trilogy and Star Wars Episode I: The Phantom Menace. For the latter, in some walking scenes, Warwick Davis incarnated Yoda as well. For the radio dramatizations of The Empire Strikes Back and Return of the Jedi, Yoda was voiced by John Lithgow, while Tom Kane voiced him in the Clone Wars animated series, several video games, and the new series Star Wars: The Clone Wars.\"\n",
    "\n",
    "# We're going to store these documents in a dictionary, which makes them easier to work with:\n",
    "documents = {'doc1':doc1}\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s56FE-bTB5-G",
    "outputId": "9c43410c-516e-46df-b9d1-2a34b7730251",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2 = \"Luke Skywalker lives a humdrum existence on Tatooine with his Uncle Owen and Aunt Beru that have kept his father's true history a secret from him. He initially wants to join the Imperial Academy to become a pilot with his childhood friend Biggs Darklighter, but is held back by his uncle who ostensibly needs his help on the moisture farm (while it was to hopefully prevent Luke from following his father's path). He takes his first steps toward his destiny when he finds the two droids C-3PO and R2-D2. After delivering R2-D2's message to hermit Ben Kenobi, Ben tells Luke that his father was a Jedi and presents him with his father's lightsaber and then tells him that his father was murdered by a traitorous Jedi. Ben offers to take Luke to the planet Alderaan and train him in the ways of the Force, but Luke rejects his offer.\"\n",
    "\n",
    "documents['doc2'] = doc2\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jT1QGAR2B5-I",
    "outputId": "2f1ec473-9199-466e-a1ee-335a0640fa09",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc3 = \"When the Empire attacks the Rebel base, Solo transports Chewbacca, along with Princess Leia and C-3PO to Cloud City where his old friend (and Cloud City administrator) Lando Calrissian operates to hide from Imperial agents. When bounty hunter Boba Fett tracks the Falcon to Cloud City, Darth Vader forces Calrissian to help capture Solo sealed in carbonite for delivery to Jabba the Hutt. But Lando frees Vader's other captives and they may rescue Solo but are too late as Fett escapes with Solo's frozen body.\"\n",
    "\n",
    "documents['doc3'] = doc3\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAoLJGeTB5-L",
    "outputId": "0eaadb50-662e-45c3-e402-33e95f5492b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc4 = \"In her first appearance in Star Wars Episode IV: A New Hope, Princess Leia is introduced as the Princess of Alderaan and a member of the Imperial Senate. Darth Vader captures her on board the ship Tantive IV, where she is acting as a spy for the Rebel Alliance. He accuses her of being a traitor and demands to know the location of the secret technical plans of the Death Star, the Galactic Empire's most powerful weapon. Unknown to Vader, the young Senator has hidden the plans inside the Astromech droid R2-D2 and has sent it to find Jedi Master Obi-Wan Kenobi on the nearby planet of Tatooine.\"\n",
    "\n",
    "documents['doc4'] = doc4\n",
    "print(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMyIV6R-B5-N",
    "outputId": "37c297e2-430f-4717-a1ec-2d1c38ee09a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc5 = \"Three years later in Star Wars Episode V: The Empire Strikes Back, Lord Vader leads an Imperial starfleet in pursuit of the Rebels. Intent on turning Luke to the dark side, Vader captures Princess Leia, Han Solo, Chewbacca and C-3PO on Cloud City to use them as bait for Luke. During a lightsaber duel, Vader cuts off Luke's right hand and reveals that he is Luke's father\"\n",
    "\n",
    "documents['doc5'] = doc5\n",
    "print(doc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW1lxkfdB5-Q",
    "outputId": "58c490b6-dcf4-452d-a13d-6b142da082b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can print the start of each document to make sure \n",
    "# that all 5 documents have been stored properly:\n",
    "for doc in sorted(documents):\n",
    "    print(f\"\\n {doc}: \\n {documents[doc][0:50]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7WJT8RqB5-U"
   },
   "source": [
    "### Boolean serach\n",
    "Boolean search looks for documents based on the presence or absence of specific search terms. The following cell defines a limited version of boolean search that you will use in subsequent cells. In those cells, you will call the function defined here by specifying a set of terms and whether the documents found should contain *all* or *any* of the terms, using the \"and\" and \"or\" operators, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbV-wfDcB5-U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please run the code in this cell without changing it - see code explanation below\n",
    "import re\n",
    "def boolean(comparison,terms,documents): # 1\n",
    "    # invert dictionary\n",
    "    documents_inverse = {value.lower():key for key,value in documents.items()} #2\n",
    "    # if we got just one term (i.e., a string), make it into a list of one\n",
    "    if type(terms)==str: # 3\n",
    "        terms = [terms]\n",
    "        \n",
    "    relevant = []\n",
    "    if comparison == \"or\": # 4\n",
    "        for d in documents: # 5\n",
    "            document = documents[d].lower()\n",
    "            rel = False\n",
    "            for term in terms:\n",
    "                # search using regular expression \\b (word boundary) operator so we only match whole words\n",
    "                if re.search(r\"\\b\" + re.escape(term.lower()) + r\"\\b\", document):\n",
    "                    rel = True\n",
    "            if rel == True:\n",
    "                relevant.append(documents_inverse[document])\n",
    "    if comparison == \"and\":\n",
    "        for d in documents:\n",
    "            document = documents[d].lower()\n",
    "            rel = True\n",
    "            for term in terms:\n",
    "                # search using regular expression \\b (word boundary) operator so we only match whole words\n",
    "                if not re.search(r\"\\b\" + re.escape(term.lower()) + r\"\\b\", document):\n",
    "                    rel = False\n",
    "            if rel == True:\n",
    "                relevant.append(documents_inverse[document])\n",
    "    \n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs17laPHAEES"
   },
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block, referencing parts in parentheses, e.g. `(1)`._\n",
    "\n",
    "(1) This line here, as we have seen before, defines a function called `boolean` that takes three arguments: `comparison`, `terms`, and `documents`. \n",
    "\n",
    "(2) This line goes ahead and inverts the dictionary `documents` that we pass into the function whenever we use it using something called list comprehension. It switches every key, value pair in the dictionary.\n",
    "\n",
    "(3) As the comment above suggestions, we check to see if the variable `terms` is a string. If it is, we go ahead and convert it to a list. The reason we do this is because the code below acts as though `terms` is always a list. If we didn't change `terms` to a list here, then each loop below would check each _character_ of the string `terms`. A string is just a list of characters! Anyway, the takeaway is that, because the code below uses `terms` as if its a list, we want to make sure it's always a list.\n",
    "\n",
    "(4) Here we have our two checks for `comparison` (well, the second one is below that checks if it equals `\"and\"`, but the blocks are rather similar). \n",
    "\n",
    "(5) The body of our if statement goes ahead and iterates through every document we've assigned above. It then uses regular expressions to find where each term shows up and appends- or adds- the document to a list called `relevant`. Then, when the function returns `relevant` as a list, we get a list of all the documents where our terms show up, depending on whether we use `\"or\"` or `\"and\"`. Nice! \n",
    "\n",
    "*for more in depth explanation see the solutions notebook or ask Mia :)*\n",
    "\n",
    "\n",
    "The cell below explains how to particularly use this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTuCX3l2B5-X"
   },
   "source": [
    "You can do Boolean retrieval by calling the above function (called `boolean`). When calling the function, you need to give it three arguments:\n",
    "\n",
    "* the operator (\"and\" or \"or\")\n",
    "* a list of terms to search for (`[\"Term1\",\"Term2\"...\"TermN\"]`)\n",
    "* the document collection to search through (in this case our dictionary called `documents`)\n",
    "\n",
    "When only one term is searched for, it goes into a list by itself. In that case, it doesn't matter whether the operator is \"and\" or \"or\".\n",
    "\n",
    "Try running the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeK0quZlB5-Y",
    "outputId": "0a5b13a7-45ab-4557-f119-3f20a711db63"
   },
   "outputs": [],
   "source": [
    "boolean(\"and\", [\"the\", \"force\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jiO4HkCB5-a"
   },
   "source": [
    "Convince yourself that the returned documents indeed contain the right query terms!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6-Vhg9gAEET"
   },
   "source": [
    "**Question 2.1** Using the Boolean retrieval model as implemented in the function `boolean`, construct a query that returns the set of documents that contain both the word \"Darth\" and the word \"Vader\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eyWc_LZAEET",
    "outputId": "fe6a6cf0-da33-4623-eaf0-c59dd8612733",
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_2_1 = boolean()\n",
    "# You can check your results by running this cell\n",
    "print(solution_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thSj4vgKAEET"
   },
   "source": [
    "**Question 2.2** Now construct a query that returns the set of documents that contain either the word \"Darth\" or the word \"Vader\" (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYIAYWi7AEET",
    "outputId": "db1322d0-f256-4088-cc9c-05fcf0eb30ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_2_2 = boolean()\n",
    "# You can check your results by running this cell\n",
    "print(solution_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Hll_w6AEET"
   },
   "source": [
    "**Question 2.3** Now construct a query that returns the set of documents that contain at least one of the following words: \"Darth\", \"Vader\", \"Luke\", \"Skywalker\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XncHsEwAEET",
    "outputId": "376631cf-ecc6-4c31-85c0-95b7fbae1de1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_2_3 = boolean()\n",
    "# You can check your results by running this cell\n",
    "print(solution_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9dyH4-gAEET"
   },
   "source": [
    "# 3. Vectors\n",
    "The  boolean  search  demonstrated  above  requires  exact  matches  and  returns  all  matching  documents.  Vector  space  models  instead  allow  partial  matching  and,  as  implemented  below,  allow  us  to  rank  the  returned  documents.  Informally,  this  model  generates  a  numeric  representation  of  each  document  and  the  query  and  computes  similarity  as  a  number  between  0  and  1.  A  document  with  a  low  score  contains  few  of  the  search  terms,  while  a  document  with  a  higher  score  contains  more  search  terms.  This  measure  of  similarity  also  normalizes  for  document  length  (accounts  for  differing  document  length  when  computing  scores).  Without  this  feature,  longer  documents  would  generally  score  higher  simply  because  they  contain  more  words  in  total.\n",
    "\n",
    "[**You can read a super simple five-minute explanation of measuring text similarity here**](https://towardsdatascience.com/introduction-to-text-representation-and-similarity-b5dd3fd71737)\n",
    "\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "**Sentence 1**: “Global warming is increasing”\n",
    "\n",
    "**Sentence 2**: “There is increasing ocean temperature”\n",
    "\n",
    "\n",
    "\n",
    "**STEP 1**: Let's pick only the most *informative* unique words from the two sentences.\n",
    "\n",
    "Unique Words: global, warming, is, increasing, there, ocean, temperature <br>\n",
    "Informative words: global, warming, increasing, ocean, temperature\n",
    "\n",
    "**STEP 2**: Count the number of occurrences of unique words in each of the sentences\n",
    "\n",
    "Analysis of sentence 1\n",
    "```\n",
    "global, 1\n",
    "warming, 1\n",
    "increasing, 1\n",
    "ocean, 0\n",
    "temperature, 0\n",
    "```\n",
    "\n",
    "Analysis of sentence 2\n",
    "```\n",
    "global, 0\n",
    "warming, 0\n",
    "increasing, 1\n",
    "ocean, 1\n",
    "temperature, 1\n",
    "\n",
    "```\n",
    "\n",
    "The easy part is over and before we proceed, you must know that NLP’s text similarity works on the basis of cosine similarity. *Cosine similarity* is basically the cosine of the angle between two vectors. So, we want to convert the sentences into two vectors, which we’ve already done!\n",
    "\n",
    "\n",
    "**Vector of sentence 1:** [1,1,1,0,0]\n",
    "\n",
    "**Vector of sentence 2:** [0,0,1,1,1]\n",
    "\n",
    "Now let’s visualize the vectors.\n",
    "\n",
    "Do note that, in our case we have a **5D** vector and because it’s not possible to visualize a 5D vector, let's just look at a **3D** vector, representing three words in our vector space. \n",
    "\n",
    "So, here we have two 3D vectors [ 1, 1, 1 ] and [ 0, 0, 1 ]. You can imagine these vectors as 2 sentences with 3 unique words in total. Here, [ 1, 1, 1 ] would mean that all 3 unique words occur once in the first sentence while [ 0, 0, 1 ] would mean that only the 3rd unique word occurs once in the second sentence.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*zXJOIv1Ouxegi-rE30fKLg.png)\n",
    "\n",
    "We’re interested only in the **angle** between these two vectors. The closer the two lines are, the smaller will be the angle and hence, similarity increases. So, If any two sentences are perfectly similar you’d see only one line in the 3D space, as the two lines would overlap each other.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:638/format:webp/1*ipF4GqzTxDGf1cZqIfglug.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*4slYq804eanHw7pqzHh23w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZCNw2Q4AEET"
   },
   "source": [
    "# Calculating cosine similarity \n",
    "Let's look at an example of how to calculate cosine similarity for two sentences given the vectors assigned to their terms. \n",
    "\n",
    "**Sentence 1**: “*Global warming* is *increasing*”\n",
    "\n",
    "**Sentences 2**: “There is *increasing ocean temperature*”\n",
    "\n",
    "\n",
    "**Vector of sentence 1:** [1,1,1,0,0]\n",
    "\n",
    "**Vector of sentence 2:** [0,0,1,1,1]\n",
    "\n",
    "\n",
    "Now we want to measure the angle between these two vectors. Here is the fomula for that: \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:350/format:webp/1*zpM7o4mYqK15J_zH9ifZpA.png)\n",
    "\n",
    "In the numerator, we have the **dot product** of the vectors and in the denominator, we have the product of the lengths of the two vectors.\n",
    "\n",
    "\n",
    "Let’s find out the **dot product** for our case:\n",
    "\n",
    "The Formula -> (u1 * v1) + (u2 * v2) + ….. + (un * vn)\n",
    "\n",
    "That’d be -> (1 * 0) + (1 * 0) + (1 * 1) + (0 * 1) + (0 * 1)\n",
    "\n",
    "Now let's find the lengths: \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*GEMt1G0_blmq-hUTkl-e2Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XfYq3oyAEET"
   },
   "source": [
    "# Now we'll do it manually in Python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UaxaMNZAEET",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Square  root  and  log  functions  needed  for  calculations:\n",
    "from  math  import  sqrt,  log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vJlJpXKAEET",
    "outputId": "126ebd7e-7674-4e14-cd04-1ac4edc58260",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get the dot product of the two vectors\n",
    "(1*0) + (1*0) +(1*1) +(0*1) +(0*1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gExt4PnJAEET",
    "outputId": "42c57541-f4d0-4878-8938-a49db140182f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get the length of the two vectors and multiply them\n",
    "sqrt(1**2+1**2+1**2+0**2+0**2)*sqrt(0**2+0**2+0**2+1**2+1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGV-du4BAEET",
    "outputId": "3450cf50-a3ef-47a4-8e4d-5ffb28f33ab3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Divide the dot product by the length of the two vectors\n",
    "1/2.4494897427831783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GokMA8KGAEET"
   },
   "source": [
    "Now let's build a function to calculate cosine similarity for us without having to go through all these steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znu8c4cLAEEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Dot  product  makes  things  a  lot  simpler  - rather than calculate this manually, we can load the dot package from numpy\n",
    "from  numpy  import  dot\n",
    "\n",
    "#We're going to treat the query itself as a vector to compute the dot product. Most terms from the docs aren't in the query, so we'll treat those as 0s!\n",
    "#  cosine  similiarity:\n",
    "def  cosine_sim(d,q):       \n",
    "    sim_un  =  dot(d,q)        \n",
    "    norm  =  sqrt(dot(d,d))*sqrt(dot(q,q))+1e-16\n",
    "    return round(sim_un/norm,3) #rounds the number within 3 decimal places\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpzGn55fAEEU"
   },
   "source": [
    "Now what we want to do is define a function <code>doc_vectors</code> that will take a query (<code>terms</code>) and a set of documents (<code>docs</code>) as input and generate weight vectors for each document based on the frequency of the terms in those documents. **Don't worry about understanding everything here but give it a try. In depth explanation can be found in the solutions notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yliBGV8ZAEEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Generate  weight  vectors  for  each  document:\n",
    "def  document_vectors(terms,docs):  \n",
    "    terms  = [term.lower() for  term  in  terms]\n",
    "    #  define  some  useful  values  used  repeatedly  in  calculations  (N  and  n  from  above):        \n",
    "    N  =  len(docs) #number of documents       \n",
    "    counts  =  [sum([1.0 for  d  in  docs  if  term  in docs[d]])  for  term  in  terms] #base score for docs in which terms occurs\n",
    "    #  pre-calculate  the  scaling  term  for  each  weight:        \n",
    "    scaling  =  [log(N/(n+1e-16),2)  for  n  in  counts]\n",
    "    #  instructions for calculating weight  vectors  for  each  document in sci. notation        \n",
    "    weights  =  {d:[1e-16]*len(terms)  for  d  in  docs}\n",
    "    #  actual  calculations\n",
    "    for  j  in  docs:                \n",
    "        document  =  docs[j].lower().split()    \n",
    "        for  i  in range(len(terms)):                       \n",
    "            frequency  = document.count(terms[i])/float(len(terms))          \n",
    "            weights[j][i]  =  frequency*scaling[i]\n",
    "    #  return  weight  vectors:\n",
    "    return  weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-S_3ez0AEEU"
   },
   "source": [
    "Now we need some code to return the results! We define another function <code>vector_query</code> which takes the same arguments (<code>terms, docs</code>) and uses <code>document_vectors</code> to return a single float between 0-1 representing the relevance of that doc given the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqnDPD6rAEEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  function  that  will  take  a  list  of  terms  and  documents  and return their cosine_sim score\n",
    "def  vector_query(terms,docs):        \n",
    "    vectors  =  document_vectors(terms,docs)\n",
    "    similarities  =  {} # we're going to store our similarity scores here\n",
    "    q  =  [1]*len(terms) #this is just returning a list of items (1s) equal to the number of terms in our query\n",
    "    for  d  in  docs:\n",
    "        similarities[d]  =  cosine_sim(vectors[d],q) #return similarity vectors for each term for each doc\n",
    "    return  similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-LI4N4pAEEU"
   },
   "source": [
    "You  can  do  Vector  Space  retrieval  by  calling  the  above  function  (called  vector_query).  When  calling  the  function,  you  need  to  give  it  two  arguments:\n",
    "\n",
    "-- a  list  of  search  terms  ([\"Term1\",\"Term2\"...\"TermN\"])\n",
    "\n",
    "-- the  document  collection  to  search  through  (in  this  case  our  dictionary  called  documents)\n",
    "\n",
    "You  can  either  type  the  seach  term  list  by  hand,  or  use  use  nltk's  \n",
    "word_tokenize  method (which maps a string to a list of words). \n",
    "\n",
    "In  the  latter  case,  you  will  need  to  provide  the  query  as  a  single  string,  and  it  will  return  the  query  as  a  tokenized  list.\n",
    "\n",
    "We'll try  running  the  following  example: \"Jedi  master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G216FF7hAEEU",
    "outputId": "75175d8a-b3c7-4553-ba52-1e29f4e4b7f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Import  the  \"word_tokenize\"  method:\n",
    "from  nltk  import  word_tokenize\n",
    "\n",
    "#  Specify  the  query  as  a  list  of  search  terms:\n",
    "terms  =  word_tokenize(\"jedi master\")\n",
    "\n",
    "#  Do  vector  space  retrieval  on  the  search  query:\n",
    "results  =  vector_query(terms,documents)\n",
    "\n",
    "#  Print  a  ranked  list  of  results:\n",
    "for  doc  in sorted(results, key=results.get, reverse=True):\n",
    "    print(doc,  results[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX-SsluVAEEU"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "What just happened here?  What do these numbers represent?  Why does Doc4 get the highest number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NVMngxUiAi2"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD6klUmcAEEU"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "Consider  the  following  queries:\n",
    "\n",
    "1.\"Luke,  I  am  your  father\"\n",
    "\n",
    "2.\"May  the  force  be  with  you\"\n",
    "\n",
    "Write  down  for  each  query  which  document  you  think  is  most  relevant for eahc query.  Simply  use  your  own  intuitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ7VkGkliB9u"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7AAm3uIAEEU"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Use  the  Vector  Space  retrieval  model  to  determine  which  documents  are  relevant  for  each  query.  \n",
    "\n",
    "Does  the  ranked  list  match  your  intuitions?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4Rs5KfZiC5X"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2krnQGDnAEEU"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Would  the  removal  of  stop  words  in  the  query  improve  the  retrieval  results?  Come  up  with  a  list  of  words  to  remove  and  show  the  results  when  you  leave  out  these  words  from  the  query.\n",
    "\n",
    "You can read more about stop-words on Wikipedia: https://en.wikipedia.org/wiki/Stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXFJGBkLiDrX"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4DPdKgkAEEU"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Which  terms  would  you  need  to  add  to  doc2  to  make  it  the  highest  ranked  document  for  the  query  \"May  the  force  be  with  you\"?  Insert  terms  in  the  document  text  (you  should  re-define  it  in  the  cell  below  rather  than  altering  the  original  definition  above)  and  show  that these changes do make it the highest-ranked document for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrO9ZvCsAEEU",
    "outputId": "0e00cebb-670d-410d-f7ce-ebd1c4e13ee5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc2  =  \"your doc2 edit here.\"\n",
    "\n",
    "documents['doc2']  =  doc2\n",
    "\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3VHRgjFiFv5"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csm_C3LoAEEU"
   },
   "source": [
    "### Question 9 (OPTIONAL!) \n",
    "\n",
    "Please write out in code OR pseudo-code a function to calculate the simplified tfidf of a word in a given document among a collection Docs. (Log optional.) For which doc among Docs does \"Force\" have the highest tf-idf?  What does that information mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J5Hy02TiG79"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JVLScWJAEEU"
   },
   "source": [
    "\n",
    "# 4. Word embeddings\n",
    "In this part we'll explore: \n",
    "- What is a word embedding?\n",
    "- What are the desired properties of word embeddings?\n",
    "- What are the potential uses of word embeddings?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU-8VMuYDDSL"
   },
   "source": [
    "\n",
    "# Understanding word embeddings conceptually \n",
    "\n",
    " [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) uses color to illustrate how word embeddings work. The pre-trained Word2Vec embeddings are based on millions of documents.\n",
    "\n",
    "![](http://jalammar.github.io/images/word2vec/word2vec.png)\n",
    "\n",
    "\n",
    "# Please read the sections of [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) called Word Embeddings and Analogies together with your group before proceeding! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vps8ifhpDALK"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Validation of Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbhwQ7KdDHpl",
    "outputId": "425c8d5a-f15c-4a8c-b551-421ee3c3c7b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install text-preprocessing\n",
    "!pip install danlp\n",
    "!pip install gensim\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "from text_preprocessing import text_preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-T-SsSODMe7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of preprocessing\n",
    "\n",
    "text_data = \"This is a sample text. We will need to split it into sentences. Afterwards, it will be split into tokens\"\n",
    "\n",
    "sentences = [sent.split() for sent in text_preprocessing.tokenize_sentence(text_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1o1HXoTDQsM",
    "outputId": "1d4a8cd0-9743-47ad-8d74-330be91eb4ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# train a word embedding \n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, \n",
    "                 vector_size=3, # size of the embedding layer (very low!)\n",
    "                 window=5, # the size of the window (max distance between current and predicted word)\n",
    "                 min_count=1, # ignore word with freq lower than this\n",
    "                 sg = 1, # should it use skip-gram (alternative is CBOW)\n",
    "                 workers=4) # number of cores to to use when training\n",
    "\n",
    "# get the word vector for a given word in the sample text\n",
    "print(model.wv[\"sample\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEfyhZPqiUkw"
   },
   "source": [
    "# Danish word embeddings\n",
    "\n",
    "---\n",
    "Now let's look at some of the examples you read about in The Illustrated Word2Vec in practice with Danish. There are a lot of examples in English online so below we work with Danish instead. \n",
    "\n",
    "(The Danish embeddings come from https://korpus.dsl.dk/resources/details/word2vec.html )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will take a minute or two\n",
    "path = \"/work/463910/compling23/HW12_material/danish_word2vec.bin\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "# English embeddings: http://vectors.nlpl.eu/repository/ (English CoNLL17 corpus)\n",
    "# eng_path = '/work/463910/compling23/HW12_material/english/english_word2vec.bin'\n",
    "# model = gensim.models.KeyedVectors.load(eng_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first explore some synonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiE2C5QFCWyf",
    "outputId": "39dbe828-9a6b-410f-bf70-1419baf321c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# most similar word (synonym detection)\n",
    "print(\"Aarhus = \")\n",
    "print(model.most_similar(positive=['aarhus'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTYECUINCwH4",
    "outputId": "d7eb926f-c0cf-4aa8-edca-c5ec42c60345",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Kat = \")\n",
    "print(model.most_similar(positive=['kat'], topn=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CXceG7GCbW9",
    "outputId": "33753372-aa51-48f2-9224-9e9a588c72b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"is corona more associated with beer or a virus:\")\n",
    "print(model.similarity('corona', 'øl'))\n",
    "print(model.similarity('corona', 'virus'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAWjZaa0CHNG",
    "outputId": "9249d071-b8e8-4676-ad74-b29f895c8827",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['rebekah'], topn=10)) #Try to run this code with YOUR name instead! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to play around with it :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QbQByHnCNRJ"
   },
   "source": [
    "## Analogies\n",
    "\n",
    "We can use basic arithmetic on word embeddings, in order to conduct word analogy task.\n",
    "\n",
    "For example:\n",
    "\n",
    "```man is to king as woman is to queen```\n",
    "\n",
    "So we can say that if we take the vector for ```king``` and subtract the vector for ```man```, we're removing the gender component from the ```king```. If we then add ```woman``` to the resulting vector, we should be left with a vector similar to ```queen```.\n",
    "\n",
    "NB: It might not be _exactly_ the vector for ```queen```, but it should at least be _close_ to it.\n",
    "\n",
    "```gensim``` has some quirky syntax that allows us to perform this kind of arithmetic.\n",
    "\n",
    "Let's look at the following example: \n",
    "\n",
    "\"Copenhagen is to Denmark what ___ is to England\" \n",
    "\n",
    "or mathemathically: $CPH - DK + UK \\approx $ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FqYlWhlCJBI",
    "outputId": "7ee3ddbd-8e5c-4a15-873f-06cc6a3f32bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#capital\n",
    "print(model.most_similar(positive=['københavn', 'england'], negative=['danmark'], topn=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPQ4mRdbCPt_",
    "outputId": "3fb576dd-3c10-4fed-e765-09ebf838a685",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maybe not?\n",
    "print(model.most_similar(positive=['københavn', 'tyrkiet'], negative=['danmark'], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoqGmvkcCQvW",
    "outputId": "a3484abc-7387-4f82-bddc-c5b902238aa9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conjugations\n",
    "print(model.most_similar(positive=['læge', 'manden'], negative=['mand'], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mET__qEtCCSz",
    "outputId": "8c3641ae-9909-42d2-cc36-53766d053863",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['mand'], negative=['kvinde'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again have some fun, play around with it - e.g., how does it do with color?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqdhz1TVB_XN"
   },
   "source": [
    "\n",
    "\n",
    "## Odd-one-out Detection\n",
    "This works by:\n",
    "- take the mean of all the word-embeddings\n",
    "- calculate the cosine-distance (similarity) from that center to each word\n",
    "- return the most dissimilar word (i.e. the one with the highest cosine-distance from that mean vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2xigb_NJB6To",
    "outputId": "ef915820-0e0e-4191-e993-f0b87ee75ed2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"sodavand brød vin juice\".split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av_V3E4_B2lA"
   },
   "source": [
    "\n",
    "Now change the code above to try some different words for odd-one-out detection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Czg2ghmVB1Pd"
   },
   "source": [
    "\n",
    "## Other ways word embeddings have been used\n",
    "\n",
    "shift in word meaning over time\n",
    "![](https://ruder.io/content/images/size/w2000/2017/10/semantic_change.png)\n",
    "\n",
    "Cross lingual word embeddings:\n",
    "![](https://s3.ap-south-1.amazonaws.com/techleerimages/771a4957-7fb8-4ddd-ba04-4fa73187e5f1.png)\n",
    "\n",
    "# Exercises:\n",
    "These exercises are made for the Danish word embedding but feel free to use the English embeddings instead (ask Mia for help if you want to use the English). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qriqwQHYBvfw"
   },
   "source": [
    "\n",
    "## Vector analogies \n",
    "\n",
    "- What is to woman (\"kvinde\") what man (\"mand\") to doctor (\"læge\")? Is this problematic?\n",
    "- Write code to do this analogy instead: Paris - France + Italy. \n",
    "  - What is the expected result based on your own knowledge? What does Word2Vec produce? \n",
    "\n",
    "- Come up with one or two other analogies to try.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUbvGTavBo_C",
    "outputId": "68de794f-b5a0-48a1-89c1-4ffdadc66ff6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doctor - man + woman\n",
    "print(model.most_similar('your code here')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eElImGguBQlI",
    "outputId": "464432b4-7889-44de-c639-6ce3aed8e68d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word - analogy <-- analogy embedding (then word + analogy embedding => analogy)\n",
    "# write your own here\n",
    "print(model.most_similar('your code here')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrShTG0hAEEV",
    "outputId": "5267cd13-5ce5-40b5-d8e1-b730f2589230",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word - analogy <-- analogy embedding (then word + analogy embedding => analogy)\n",
    "# write your own here\n",
    "print(model.most_similar('your code here')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRv7RpYUhObA",
    "tags": []
   },
   "source": [
    "## Plurals and sentiment \n",
    "- Discuss how you would find plurals of a Danish word\n",
    "    - Find plurals of 3 danish words using word embeddings\n",
    "- Discuss how you would find the antonym of a Danish word\n",
    "- Examine 3 words with multiple meaning, how well does word embeddings handle these?\n",
    "- Load this tagged [data](https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-da-32.txt) (code provided). This is from a Danish sentiment lexicon from AFINN tagged by Finn A. Nielsen (A Danish NLP researcher). Discuss how you could expand this lexicon using word embeddings and test out if your assumptions are correct.\n",
    "- Can you use odd one out detection on AFINN's sentiment lexicon to check for errors in the dictionary? I.e. words which are tagged as positive but are in fact not?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "import csv\n",
    "\n",
    "path = '/work/463910/compling23/HW12_material/AFINN-da-32.txt'\n",
    "with open(path) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    afinn = list(reader)\n",
    "    \n",
    "afinn[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell for answering the Q's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf3ElynOAEEV"
   },
   "source": [
    "# 5. PageRank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZeyhHGOAEEV"
   },
   "source": [
    "Based on a gist by Sebastien Bratieres and on the Wikipedia page for PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO8mBUtYAEEV"
   },
   "source": [
    "PageRank was the website sorting algorithm that cemented Google's early success in the 1990s, when it displayed webpages with the highest PageRank first. Larry Page and Sergey Brin, the co-founders of Google, developed PageRank while they were PhD students at Stanford University in 1996 as part of a research project. PageRank is named both after Larry Page and after the term \"web page\". Its main idea is that important webpages are likely to receive many links from other important webpages. The PageRank algorithm can be seen as computing the probability that a person who randomly clicks on links and follows them will arrive at a particular website.  (In case you're wondering if this assumes that people keep clicking around, PageRank actually includes a dampening factor that represents the probability that a person will actually keep clicking as opposed to stopping where they are. It’s usually set to 0.85.)\n",
    "\n",
    "It used to be the case that developers could see their page's PageRank via a tool that Google provided. Although this public score has since then been discontinued, PageRank is [still a part of Google's algorithm](https://ahrefs.com/blog/google-pagerank/).\n",
    "\n",
    "Incoming links lift a website's rank if they come from higher-ranked sites, particularly if these sites have few outgoing links. The following slightly creepy graphic shows this. The higher a website's rank (as indicated by the number of size of websites pointing at it), the bigger it is and the bigger its smile is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v95lMIMvAEEV"
   },
   "source": [
    "![PageRank cartoon](https://upload.wikimedia.org/wikipedia/commons/6/69/PageRank-hi-res.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luUwJvfdAEEV"
   },
   "source": [
    "_(Graphic by Felipe Micaroni Lalli, distributed under a [CC BY-SA 2.5](https://creativecommons.org/licenses/by-sa/2.5/deed.en) license. Source: https://upload.wikimedia.org/wikipedia/commons/6/69/PageRank-hi-res.png)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6Sj4Oc_AEEV"
   },
   "source": [
    "Here we will work with a toy example that involves just eleven websites, which link to each other as shown here. In the graph below, the percentages and the sizes of the circles represent the page ranks of these sites. Let's look at an implementation of the PageRank algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfbvW-K3AEEV"
   },
   "source": [
    "![PageRank illustration](http://upload.wikimedia.org/wikipedia/commons/f/fb/PageRanks-Example.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKq7DLRnAEEV"
   },
   "source": [
    "_(Graphic by 345Kai, placed into the public domain. Source: http://upload.wikimedia.org/wikipedia/commons/f/fb/PageRanks-Example.svg)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnOtDisBAEEV"
   },
   "source": [
    "First, we will encode the links present on this graph as a count matrix `M_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcKCs_9eAEEV",
    "outputId": "f626ddb5-949e-4ac0-95d8-695c1bf4674e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### THIS CELL DEFINES THE GRAPH ABOVE - DO NOT EDIT THIS CELL\n",
    "import numpy as np\n",
    "n_pages = 11 # numbering pages A through K as 0 to 10\n",
    "M_counts = np.zeros((n_pages, n_pages)) # will hold the number of link counts (assumed 0 or 1)\n",
    "# columns = starting page, row = destination page, ie M_ij = whether or not there is a link from j to i\n",
    "\n",
    "# create variable names for each page index for clarity\n",
    "# 1\n",
    "pageA = 0\n",
    "pageB = 1\n",
    "pageC = 2\n",
    "pageD = 3\n",
    "pageE = 4\n",
    "pageF = 5\n",
    "pageG = 6\n",
    "pageH = 7\n",
    "pageI = 8\n",
    "pageJ = 9\n",
    "pageK = 10\n",
    "\n",
    "# 2\n",
    "M_counts[:,pageA] = 1 # page 0 (A in the graphic) is a sink because it has no outgoing links at all; \n",
    "# however, M cannot contain an all-zero column, so do as if A was linking to all other pages (ie put 1's everywhere)\n",
    "M_counts[pageA,pageD] = 1  # to A from D\n",
    "M_counts[pageB,pageC] = 1  # to B from C\n",
    "M_counts[pageB,pageD] = 1  # to B from D\n",
    "M_counts[pageB,pageE] = 1  # to B from E\n",
    "M_counts[pageB,pageF] = 1  # to B from F\n",
    "M_counts[pageB,pageG] = 1  # to B from G\n",
    "M_counts[pageB,pageH] = 1  # to B from H\n",
    "M_counts[pageB,pageI] = 1  # to B from I\n",
    "M_counts[pageC,pageB] = 1  # to C from B\n",
    "M_counts[pageD,pageE] = 1  # to D from E\n",
    "M_counts[pageE,pageF] = 1  # to E from F\n",
    "M_counts[pageE,pageG] = 1  # to E from G\n",
    "M_counts[pageE,pageH] = 1  # to E from H\n",
    "M_counts[pageE,pageI] = 1  # to E from I\n",
    "M_counts[pageE,pageJ] = 1  # to E from J\n",
    "M_counts[pageE,pageK] = 1  # to E from K\n",
    "M_counts[pageF,pageE] = 1  # to F from E\n",
    "\n",
    "print(M_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RqreoAgAEEV"
   },
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block, referencing parts in parentheses, e.g. `(1)`._\n",
    "\n",
    "(1) First, we go ahead and make a bunch of variables for each page set to numbers. This is mostly for code readability. Much clearer to say `pageA` links to `pageD` with the line `M_counts[pageA,pageD]` than it is to write something like `M_counts[0,3]`!\n",
    "\n",
    "(2) Then we go ahead and initialize values for what pages are linked to each other. In the grid that prints out below, anywhere there's a `1` represents where a page is linked. The first column is all `1s` because each page will link back to A by default. Somewhere like `[1,3]` will be a 1 to represent page 1 linked to page 3- or as defined above, pageB linked to pageD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgcxiT7HAEEV"
   },
   "source": [
    "Now we convert  `M_counts` to `M` below, by dividing each column by its sum, i.e. we are making sure columns sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBYNqPDKAEEV",
    "outputId": "669aa082-2b24-463f-9823-88761ed4ecef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.empty((n_pages, n_pages))\n",
    "for j in range(n_pages):\n",
    "    M[:,j] = M_counts[:,j] / M_counts[:,j].sum()\n",
    "np.set_printoptions(precision=3)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o22FVoJAEEV"
   },
   "source": [
    "The following code checks that `M` has the right format to be used by the PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnNwsvsiAEEV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_M(M):\n",
    "    \"\"\"\n",
    "    check that M has the right format to be used by pagerank function\n",
    "    \"\"\"\n",
    "    n_pages = M.shape[0] # n_pages is the number of rows of M\n",
    "    np.testing.assert_equal(M.shape[0], M.shape[1], err_msg = 'M should be square')\n",
    "    np.testing.assert_array_almost_equal(M.sum(axis=0), np.ones((n_pages)), \n",
    "                                         err_msg = 'assert each column sums to one (M is assumed column-stochastic)')\n",
    "    for j in range(n_pages):\n",
    "        M_column = M[:,j]\n",
    "        n_nonzero = np.count_nonzero(M[:,j])\n",
    "        np.testing.assert_array_almost_equal(M_column[M_column.nonzero()], np.ones((n_nonzero)) / n_nonzero,\n",
    "                                             err_msg = 'in column %g, all non-zero entries should be equal (and equal to 1 divided by their number)' % j)\n",
    "\n",
    "check_M(M) # will produce error if M does not have the right format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t-1jZ4IAEEV"
   },
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code block._\n",
    "\n",
    "This code block uses the library `numpy`, as imported in a cell way above with `import numpy as np`. Where you see `np.testing.assert...` represents the numpy library making sure something is true. For example, the line below `n_pages = M.shape[0]` is testing if the first argument is equal to the second argument. Namely, if the shape (or dimensions) is a square. `M.shape[0]` gets the number of rows, and `M.shape[1]` gets the number of columns.\n",
    "\n",
    "If you're curious, print out `M.shape`. You should see something like `(x,y)`. Thus, `M.shape[0]` would print the same value as `x`, and similarly for `M.shape[1]` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dql9VMwxAEEW"
   },
   "source": [
    "We are now ready to apply the pagerank function, which will iteratively apply page transitions to an randomly initialized distribution over the pages, until we converge. The details of the PageRank algorithm do not matter for this exercise beyond what was described in the textbook; however, if you are interested, the [Wikipedia page on PageRank](https://en.wikipedia.org/wiki/PageRank) gives a good overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEYI61boAEEW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pagerank(M, d=0.85, square_error=1e-8):\n",
    "    \"\"\"\n",
    "    M : the adjacency matrix of the pages. It is assumed to be column-stochastic (i.e. column sum to 1); all links have equal weight.\n",
    "    A page with no outgoing links (sink) is represented as a page with outgoing links to each other page (ie restart page).\n",
    "    d: damping factor (this is not in the textbook, but commonly applied in PageRank implementations; it represents the probability that a user stops clicking)\n",
    "    square_error : the algorithm iterates until the difference between two successive PageRank vectors v is less than this (in squared norm)\n",
    "    returns the PageRanks of all pages\n",
    "    \"\"\"\n",
    "    n_pages = M.shape[0] # n_pages is the number of rows of M\n",
    "    v = np.random.rand(n_pages) # initialize to random vector\n",
    "    v = v / v.sum() # make v sum to 1\n",
    "    last_v = np.ones((n_pages)) # will contain the previous v\n",
    "    M_hat = d * M + (1-d)/n_pages * np.ones((n_pages, n_pages)) # this is the central PageRank equation\n",
    "    while np.square(v - last_v).sum() > square_error:\n",
    "        last_v = v\n",
    "        v = M_hat.dot(v) # at each iteration, progress one timestep\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYUlDuihAEEW",
    "outputId": "989e5f8e-f059-47a3-86e5-3c814f5463aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = pagerank(M)\n",
    "print(\"Result of running PageRank on M:\")\n",
    "site_names = \"ABCDEFGHIJK\"\n",
    "for i in range(10):\n",
    "    print(\"Page\", site_names[i]+\":\", \"{:.1%}\".format(result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBH2Gr4tAEEW"
   },
   "source": [
    "#### Code Explanation\n",
    "_The following explanation refers to the above code blocks._\n",
    "\n",
    "The first block, that defines our `pagerank` function, returns a list that is the pagerank of all pages- which you can see as the variable `result` in our second codeblock. The math tricks in the first block aside, the `pagerank` function gives you a list which is then printed in the second block. The higher the percentage, the more relevant they are when you search up something related to these websites (or pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLb8EX2LAEEW"
   },
   "source": [
    "These are the numbers (within the allowed error) displayed on the graph (the numbers on the graph are rounded exact values). There might be slight discrepancies because of how the algorithm depends on random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v36rEo3hB5-c"
   },
   "source": [
    "Suppose the webmaster of website B edits it to include a link to website A (so, B is now linking to both A and C). Look at the graphic above. Which site do you think will now have the highest PageRank? Will A continue to be ranked below C? (No need to submit an answer at this point, just keep the answer in mind.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHvY3Zr_AEEW"
   },
   "source": [
    "The following code is an exact copy of the code above, except that the variables have been renamend (instead of M_counts there is M_new_counts etc.), and in the first cell there are instructions to add a new link to A coming from website B. Add this link (<font color=\"red\">remember, it is a link **to** A **from** B, not the other way around; note that in the code, all links are given in the format \"to X from Y\"; that is, it is counter to what people might normally intuit</font>). Then run that cell and all of the subsequent cells in preparation for the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jOdjCC2AEEW",
    "outputId": "cc8fb853-6e10-4268-f41a-ae9e2de0eb82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### DUPLICATE OF CODE ABOVE -- EDIT THIS CELL AS INDICATED BELOW\n",
    "import numpy as np\n",
    "n_pages = 11 # numbering pages A through K as 0 to 10\n",
    "M_new_counts = np.zeros((n_pages, n_pages)) # will hold the number of link counts (assumed 0 or 1)\n",
    "# columns = starting page, row = destination page, ie M_ij = whether or not there is a link from j to i\n",
    "\n",
    "# crate variable names for each page index for clarity\n",
    "pageA = 0\n",
    "pageB = 1\n",
    "pageC = 2\n",
    "pageD = 3\n",
    "pageE = 4\n",
    "pageF = 5\n",
    "pageG = 6\n",
    "pageH = 7\n",
    "pageI = 8\n",
    "pageJ = 9\n",
    "pageK = 10\n",
    "\n",
    "M_new_counts[:,pageA] = 1 # page 0 (A in the graphic) is a sink because it has no outgoing links at all; \n",
    "# however, M cannot contain an all-zero column, so do as if A was linking to all other pages (ie put 1's everywhere)\n",
    "M_new_counts[pageA,pageD] = 1  # to A from D\n",
    "M_new_counts[pageB,pageC] = 1  # to B from C\n",
    "M_new_counts[pageB,pageD] = 1  # to B from D\n",
    "M_new_counts[pageB,pageE] = 1  # to B from E\n",
    "M_new_counts[pageB,pageF] = 1  # to B from F\n",
    "M_new_counts[pageB,pageG] = 1  # to B from G\n",
    "M_new_counts[pageB,pageH] = 1  # to B from H\n",
    "M_new_counts[pageB,pageI] = 1  # to B from I\n",
    "M_new_counts[pageC,pageB] = 1  # to C from B\n",
    "M_new_counts[pageD,pageE] = 1  # to D from E\n",
    "M_new_counts[pageE,pageF] = 1  # to E from F\n",
    "M_new_counts[pageE,pageG] = 1  # to E from G\n",
    "M_new_counts[pageE,pageH] = 1  # to E from H\n",
    "M_new_counts[pageE,pageI] = 1  # to E from I\n",
    "M_new_counts[pageE,pageJ] = 1  # to E from J\n",
    "M_new_counts[pageE,pageK] = 1  # to E from K\n",
    "M_new_counts[pageF,pageE] = 1  # to F from E\n",
    "### YOUR TASK: ADD A LINE TO THE ABOVE LIST THAT REPRESENTS A NEW LINK TO PAGE A FROM PAGE B\n",
    "\n",
    "print(M_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yi2a-RN6AEEW",
    "outputId": "bb208aa7-17b2-49f1-ffe7-d52e4253be4a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### DUPLICATE OF CODE ABOVE, BUT WITH M_new instead of M and M_new_counts instead of M_counts\n",
    "M_new = np.empty((n_pages, n_pages))\n",
    "for j in range(n_pages):\n",
    "    M_new[:,j] = M_new_counts[:,j] / M_new_counts[:,j].sum()\n",
    "np.set_printoptions(precision=3)\n",
    "print(M_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-c1uyEwAEEW",
    "outputId": "e6a2cf33-51d8-43ff-f5e9-4e1a103ab8f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### DUPLICATE OF CODE ABOVE, BUT WITH M_new instead of M and result_new instead of result\n",
    "result_new = pagerank(M_new)\n",
    "print(\"Result of running PageRank on M_new:\")\n",
    "site_names = \"ABCDEFGHIJK\"\n",
    "for i in range(10):\n",
    "    print(\"Page\", site_names[i]+\":\", \"{:.1%}\".format(result_new[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWEqJDsuAEEW"
   },
   "source": [
    "For this and the following questions, check the result against your intuitions before you submit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcpGH1uEAEEW"
   },
   "source": [
    "**Question 3.1** What is the percentage value of the top ranked website? Give the result as a decimal number between 1 and 100, rounded to one decimal, without the \"percent\" sign. Do not divide by 100. This question and the next one can be answered by checking the lists in the output above. It is there for us to see whether you have run the PageRank algorithm correctly as per our instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1L6YBhZ9AEEW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_3_1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_pjEFnZAEEW"
   },
   "source": [
    "**Question 3.2 (10 points)** And what is the percentage value of the second highest ranked website? Give the result as a decimal number between 1 and 100, rounded to one decimal, without the \"percent\" sign. Do not divide by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuyKQrtVAEEW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution_3_2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InoGUD7zAEEW"
   },
   "source": [
    "## Background on search engine crawlers\n",
    "Search engines \"crawl\" (i.e. look through) the web every few days or weeks and use the result to compute PageRank and similar measures. When you search for a term, the search engine returns sites ranked according to the index it has stored at that moment in time, which may be out of date in case there has been a change to some website. The following scenario tests your ability to understand the concept of crawling and to integrate this information with what you have just learned from practicing with the PageRank algorithm. \n",
    "- Suppose the index of a search engine is initially as above, before you made the change. Assume that the websites A, C, E, and F mention the search term \"skiing\" and that no other websites do. \n",
    "- On Tuesday, Alice, the webmaster of B, adds a link to A to her site as described above. The search engine does not know of this change yet. \n",
    "- On Wednesday, Bob searches for \"skiing\". \n",
    "- On Thursday, the search engine crawler visits Alice's website for the first time this week, and stores an updated index that reflects the change Alice has made to her website. The search engine then reruns PageRank as described above. \n",
    "- On Friday, Carol searches for \"skiing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DwB9BytqAEEW"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.3.1**. What is the top result of Bob's search for the term \"skiing\"? In other words, which of sites A, C, E, and F (that is, which of those sites that mention the term) is ranked highest according to the index on Wednesday? Please state your answer as a single uppercase letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paz6gCz_AEEW"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9AJaaDcgAEEW"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.3.2**. Briefly explain your reasoning. Why is the top result for Bob what you said it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi_PEHH3AEEW"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "X-JaIBB9AEEX"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.3.3**. What is the top result of Carol's search for the term \"skiing\"? In other words, which of sites A, C, E, and F (that is, which of those sites that mention the term) is ranked highest according to the index on Friday? Please state your answer as a single uppercase letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe9uLd2MAEEX"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bSdhn5rMAEEX"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.3.4**. Briefly explain your reasoning. Why is the top result for Carol what you said it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5quBjogAEEX"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "otter": {
   "tests": {
    "q0.name-and-netID": {
     "name": "q0.name-and-netID",
     "points": 0,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(FIRST_NAME) == str\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(LAST_NAME) == str\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(NYU_NET_ID) == str\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> FIRST_NAME != \"\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> LAST_NAME != \"\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> NYU_NET_ID != \"\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> not NYU_NET_ID.startswith(\"N\")\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.1": {
     "name": "q1.1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (type(solution_q1_1) == float or type(solution_q1_1) == int) and (0 <= solution_q1_1 and solution_q1_1 <= 1)\nTrue",
         "failure_message": "Your solution should be either a fraction or a decimal, and must not be enclosed in quotation marks. It should be between 0 and 1, since it is a probability.",
         "hidden": false,
         "locked": false,
         "success_message": "You have entered your solution in the correct format. Whether it is the right solution will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (type(solution_q1_2) == float or type(solution_q1_2) == int) and (0 <= solution_q1_2 and solution_q1_2 <= 1)\nTrue",
         "failure_message": "Your solution should be either a fraction or a decimal, and must not be enclosed in quotation marks. It should be between 0 and 1, since it is a probability.",
         "hidden": false,
         "locked": false,
         "success_message": "You have entered your solution in the correct format. Whether it is the right solution will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> obj=solution_2_1\n>>> bool(obj) and all(isinstance(elem, str) for elem in obj)\nTrue",
         "failure_message": "It doesn't appear your solution is a list of strings. Make sure your word choices are correct based on the instructions.",
         "hidden": false,
         "locked": false,
         "success_message": "Your solution successfully finds a list of strings of document names. Whether or not these values are CORRECT will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> obj=solution_2_2\n>>> bool(obj) and all(isinstance(elem, str) for elem in obj)\nTrue",
         "failure_message": "It doesn't appear your solution is a list of strings. Make sure your word choices are correct based on the instructions.",
         "hidden": false,
         "locked": false,
         "success_message": "Your solution successfully finds a list of strings of document names. Whether or not these values are CORRECT will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> obj=solution_2_3\n>>> bool(obj) and all(isinstance(elem, str) for elem in obj)\nTrue",
         "failure_message": "It doesn't appear your solution is a list of strings. Make sure your word choices are correct based on the instructions.",
         "hidden": false,
         "locked": false,
         "success_message": "Your solution successfully finds a list of strings of document names. Whether or not these values are CORRECT will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(solution_3_1)==float\nTrue",
         "failure_message": "Your solution should be a decimal, and must not be enclosed in quotation marks.",
         "hidden": false,
         "locked": false,
         "success_message": "You have entered your solution in the correct format. Whether it is the right solution will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(solution_3_2)==float\nTrue",
         "failure_message": "Your solution should be a decimal, and must not be enclosed in quotation marks.",
         "hidden": false,
         "locked": false,
         "success_message": "You have entered your solution in the correct format. Whether it is the right solution will be checked after you submit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
